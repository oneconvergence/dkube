name: dkube-job
description: |
    Defines the component spec of a dkube job.
    Dkube job is an abstract definition of different job types that can run on Dkube platform.
    Job type can be Preprocessing/Training/Serving/Custom

    PreprocessingJob - 
        Provides data preprocessing on Dkube platform.
        Provides,
        * Ability to orchestrate and run custom containers.
        * Renders utilization graphs for CPU, Memory.
        * Tags to group related preprocessing jobs.

    TrainingJob - 
        Provides training for ML/DL models on Dkube platform.
        Dkube training offers,
        * Advanced options for distributed training, gpu managment & pooling.
        * Support Hyper parameter tuning.
        * MPI operator support for Horovod like training programs.
        * Ability to orchestrate and run custom training containers, prebuilt dkube datascience containers can also be used.
        * Renders nice Dashboard for training metrics and utilization graphs for GPU, CPU, Memory.
        * Support for early stopping if program is not converging - User can abort the Job and resume from previous point in training.
        * Tags to group related training jobs.

    ServingJob -
        Provides serving for a trained model.
        Dkube serving provides,
        * Option to deploy with CPU/GPU.
        * A web server in the front and all the required infra to access the server.
        * Deployed as microservice. Serving URL is provided for any other application logic to consume the APIs for inference.
        * Support for custom serving - user can pass any custom container and get access to dkube artifacts.

    CustomJob -
        Provides an interface for launching custom job on DKube.
        User can pass the container, args and policy to expose the customjob as service for external applications or other microservices.

metadata:
  annotations: {platform: 'Dkube'}
  labels: {platform: 'Dkube', wfid: '{{workflow.uid}}', runid: '{{pod.name}}'}
inputs:
  - {name: name,            type: String,   optional: false,
     description: 'Required. Name of the dkube job.'}

  - {name: authtoken,      type: String,   optional: false,
     description: 'Required. Dkube authentication token.'}

  - {name: jobclass,        type: String,   optional: false,
     description: 'Required. DKube job to be launched, allowed values: training/data/serving/custom'}

  - {name: training,        type: Dict,     optional: true,     default: '{}',
     description: 'Optional. Training job definition filled and returned by dkube.rsrcs:DkubeTraining().to_dict()'}

  - {name: preprocessing,   type: Dict,     optional: true,     default: '{}',
     description: 'Optional. Preprocessing job definition filled and returned by dkube.rsrcs.DkubePreprocessing().to_dict()'}

  - {name: serving,         type: Dict,     optional: true,     default: '{}',
     description: 'Optional. Serving job definition filled and returned by dkube.rsrcs.DkubeServing().to_dict()'}

  - {name: visualizer,      type: Dict,     optional: true,     default: '{}',
     description: 'Optional. Custom visualizer for this job, should be a self container container image with entrypoint set. 
                   Artifact storage will be shared between this visualizer and job.
                   which visualizer code can use to render. Visualizer should always render @ /.
                   Format: {"name": <>, "image": <url>, "user:<>, "password":<>
                   name - Display name of the visualizer
                   image - Self contained image name
                   user, password - Credentials to download the image'}
outputs:
  - {name: rundetails,      description: 'Details of the dkube job run'}
  - {name: artifacts,       description: 'Dict of {name:version} of all the artifacts generated by the job'}

implementation:
  container:
    image: ocdr/dkube_launcher:py
    command: ['dkube']
    args: [
      --name, {inputValue: name}, 
      --token, {inputValue: authtoken},
      --command, {inputValue: jobclass},
      --training, {inputValue: training},
      --preprocessing, {inputValue: preprocessing},
      --serving, {inputValue: serving},
      --visualizer, {inputValue: visualizer},
      --runid, '{{pod.name}}',
      --workflowid, '{{workflow.uid}}'
    ]
    fileOutputs:
      rundetails:   /tmp/rundetails
      artifacts:    /tmp/artifacts
